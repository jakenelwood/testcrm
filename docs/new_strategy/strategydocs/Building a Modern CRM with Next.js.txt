A Founder's Playbook for Building an AI-Powered CRM on the
Modern Web Stack

I. Deconstructing the Vision: The AI-Powered CRM as a
Collaborative Partner
The development of a new Customer Relationship Management (CRM) system
requires a foundational vision that extends beyond the traditional role of a data
repository. The objective is to create an intelligent system where the CRM acts as an
abstraction layer for the end-user and functions as an AI partner. This system must be
engineered to leverage data to drive sales activity, enhance clarity for the user, and
facilitate a cycle of continual learning. This initial section translates this ambitious
vision into a concrete product strategy, defining the core principles that will guide
every subsequent technical decision.

A. Defining the Abstraction: Beyond a System of Record
The core strategic decision that will differentiate this CRM is a fundamental shift in its
operational paradigm. Traditional CRM systems function as a "system of record,"
essentially a user interface for a database that requires users to manually pull
information through searches, filters, and report generation.1 This model places the
cognitive burden on the user to know what questions to ask and where to look for the
answers. The envisioned system inverts this model, operating on a proactive
push principle. It is designed to anticipate user needs and push relevant intelligence,
insights, and suggestions directly to them. The CRM thus becomes an abstraction
layer, hiding the raw complexity of database tables and rows and instead surfacing
actionable intelligence.2
This "Pull-to-Push Paradigm Shift" is the most significant strategic choice in the
product's design. It dictates that the primary user interface cannot be a static list of
contacts or a conventional data table. Instead, the central user experience must be a
dynamic, personalized dashboard that proactively answers the implicit user question:
"What should I know and do right now to be most effective?" This transformation from
a passive repository to an active partner is the key market differentiator. The user's

goal of creating an "abstraction" is achieved through this shift. A partner anticipates
needs and provides information proactively, which means the UI must be architected
around AI-curated "briefings," "alerts," and "suggested actions" rather than simple
database views. This reframes the product from a mere tool into an intelligent service.

B. The User & AI Partnership: A Framework for Collaboration
To build an effective AI partner, it is essential to clearly define the roles and
responsibilities within this human-machine collaboration. The framework for this
partnership assigns distinct tasks based on the unique strengths of both the human
user and the AI system. The AI is tasked with handling the rote, computational, and
data-intensive work, thereby freeing the user to concentrate on high-value activities
that require uniquely human skills.
●​ The User's Role: The user remains the strategic driver of the sales process. Their

responsibilities include building rapport with clients, engaging in creative
problem-solving during negotiations, making final strategic decisions, and
ultimately closing deals. These are tasks that rely on empathy, intuition, and
complex social understanding.
●​ The AI's Role: The AI partner serves as a powerful assistant, excelling at tasks
that involve scale, speed, and pattern recognition. Its primary functions include
analyzing vast amounts of data from customer interactions, identifying subtle
patterns that predict success, automating repetitive tasks such as data entry and
the initial drafting of follow-up communications, and providing data-driven
decision support to the user.3
This collaborative model must be explicitly manifested in the application's design to
be effective. The UI should not treat the AI as a hidden background process but as a
visible and interactive teammate. This can be achieved by designing specific,
dedicated UI components that surface the AI's contributions. For example, the
dashboard could feature an "AI Daily Briefing" widget, a "Suggested Next Actions" list
for each deal, and "AI-Generated Draft" buttons within communication workflows. This
approach makes the AI's work tangible, allowing the user to interact with, approve, or
modify its suggestions. This interaction model reinforces the partnership dynamic,
building user trust and encouraging adoption by presenting the AI's outputs not as
infallible commands but as helpful recommendations from a capable assistant.
Research into AI's role in sales automation confirms its effectiveness in handling
repetitive tasks like drafting emails and identifying upsell opportunities, making these

UI components directly aligned with proven use cases.5

C. Your Three Pillars of Intelligence: The Core AI Feature Set
To structure the development of the AI partner, the core feature set can be organized
into three distinct pillars of intelligence. Each pillar addresses a fundamental user
need and contributes to the overarching goals of driving sales activity, providing
clarity, and fostering continual learning. This framework provides a clear and powerful
product roadmap, ensuring that AI development is focused and maps directly to user
value.
●​ Pillar 1: Predictive Intelligence (The Oracle): This pillar is focused on answering

the critical question, "Where should I focus my time for the highest impact?" The
flagship feature for this pillar is Predictive Lead Scoring. This capability involves
using machine learning models to analyze historical data—such as lead
demographics, engagement patterns, and past deal outcomes—to calculate a
numerical score for each new lead. This score represents the lead's likelihood to
convert into a paying customer, allowing sales professionals to prioritize their
efforts on the most promising opportunities and manage their pipeline more
efficiently.8 This directly addresses the goal of driving sales activity by intelligently
directing user effort.
●​ Pillar 2: Generative Intelligence (The Assistant): This pillar addresses the
user's need for efficiency, answering the question, "How can I execute my tasks
faster and more effectively?" The key features here leverage the power of large
language models (LLMs) for content creation and summarization. This includes
AI-powered summarization of lengthy call transcripts, email threads, and
meeting notes to provide users with a quick understanding of past interactions. It
also includes generative drafting of personalized follow-up emails, proposals,
and other communications, which the user can then review, edit, and send.3 This
pillar provides clarity on past interactions and dramatically accelerates the
execution of daily sales tasks.
●​ Pillar 3: Interactive Intelligence (The Analyst): This pillar empowers the user to
explore their data and answer ad-hoc questions, fulfilling the need for "continual
learning" by answering the question, "What's the story here?" The core feature
for this pillar is a Natural Language Interface to the Database (NLIDB). This
technology allows users to ask questions about their data in plain English (e.g.,
"Show me all deals over $10,000 that are in the negotiation stage in the EMEA
region") and receive an immediate, accurate answer. This eliminates the need for
users to understand complex query languages or navigate cumbersome reporting

tools, making data exploration intuitive and accessible.12
Together, these three pillars form a comprehensive intelligence framework. The
system addresses the past (generative summarization), guides the present (predictive
lead scoring), and enables future exploration (interactive NLIDB). This strategic
narrative ensures that each AI feature is not just a technical novelty but a purposeful
component designed to deliver tangible value to the user.

II. The Foundational Layer: Architecting Your Data Core with
Supabase and Drizzle

The bedrock of any successful CRM—especially one powered by AI—is its data
architecture. A well-designed, scalable, and secure data layer is a prerequisite for
both high-performance application functionality and the effectiveness of the
intelligence engine. This section details the process of building this foundation using a
modern stack composed of PostgreSQL, Supabase, and Drizzle ORM.

A. Practical PostgreSQL Schema Design for a Modern CRM
The design of the database schema must be approached with both current needs and
future scalability in mind. Following the principle of "start with the essentials" helps to
avoid over-engineering, a common pitfall in early-stage product development.21 The
initial schema must be minimal yet robust, and critically, it must be designed from the
outset to capture the specific data points that the AI features will rely upon for
training and inference.
A semi-technical founder requires a concrete and reliable starting point. The following
schema provides a ready-to-use SQL script that defines the essential tables, columns,
data types, and relationships for a foundational CRM. This practical asset saves
significant research and design time and is structured to support the three pillars of
intelligence defined previously. The schema includes tables for contacts, companies,
deals (opportunities), and interactions, with foreign key relationships linking them
together in a logical, normalized structure.21

Table Definition

Description

companies

Stores information about organizations.

id uuid primary key default gen_random_uuid()

Unique identifier for the company.

name text not null

The legal name of the company.

domain text unique

The company's primary website domain, used
for enrichment.

industry text

The industry sector the company operates in.

created_at timestamp with time zone default
timezone('utc'::text, now()) not null

Timestamp of when the record was created.

contacts

Stores information about individual people.

id uuid primary key default gen_random_uuid()

Unique identifier for the contact.

first_name text

The contact's first name.

last_name text

The contact's last name.

email text unique not null

The contact's primary email address.

phone text

The contact's phone number.

company_id uuid references companies(id) on
delete set null

Foreign key linking to the contact's company.

owner_id uuid references auth.users(id) on
delete cascade not null

Foreign key linking to the user who owns this
contact.

created_at timestamp with time zone default
timezone('utc'::text, now()) not null

Timestamp of when the record was created.

deals

Stores information about sales opportunities.

id uuid primary key default gen_random_uuid()

Unique identifier for the deal.

name text not null

A descriptive name for the deal.

stage text not null default 'Prospecting'

The current stage in the sales pipeline (e.g.,
Prospecting, Qualified, Closed Won, Closed
Lost).

value numeric(12, 2)

The monetary value of the deal.

close_date date

The expected date the deal will close.

company_id uuid references companies(id) on
delete cascade

Foreign key linking to the associated company.

contact_id uuid references contacts(id) on
delete set null

Foreign key linking to the primary contact for
the deal.

owner_id uuid references auth.users(id) on
delete cascade not null

Foreign key linking to the user who owns this
deal.

created_at timestamp with time zone default
timezone('utc'::text, now()) not null

Timestamp of when the record was created.

interactions

Stores records of all touchpoints with contacts
and deals. This is the AI goldmine.

id uuid primary key default gen_random_uuid()

Unique identifier for the interaction.

type text not null

The type of interaction (e.g., 'Email', 'Call',
'Meeting').

content text

The raw text content (email body, call
transcript, meeting notes).

date timestamp with time zone not null

The date and time of the interaction.

contact_id uuid references contacts(id) on
delete cascade

Foreign key linking to the associated contact.

deal_id uuid references deals(id) on delete set
null

Foreign key linking to the associated deal.

owner_id uuid references auth.users(id) on
delete cascade not null

Foreign key linking to the user who conducted
the interaction.

sentiment_score numeric(3, 2)

A score from -1 to 1 representing the sentiment,
generated by AI.

is_ai_summarized boolean default false

A flag indicating if an AI summary has been
generated for this interaction.

summary text

The AI-generated summary of the interaction
content.

The interactions table is the most critical data asset for the AI partner. It serves as the
raw material for generative summarization, sentiment analysis, and the identification
of winning sales patterns through predictive modeling.3 The design of this table is
intentionally forward-looking. It includes not only a
content field for unstructured text but also structured metadata fields like
sentiment_score, is_ai_summarized, and summary. Capturing this richer, structured
data from the start future-proofs the application and makes the development of
advanced AI features exponentially more efficient.

B. Setting Up Supabase: Your Backend-as-a-Service Powerhouse
For a solo founder or a small team, a Backend-as-a-Service (BaaS) platform like
Supabase is a massive strategic advantage. It abstracts away the immense complexity
of managing server infrastructure, database administration, and authentication
systems. By providing a managed PostgreSQL database, a secure authentication
service, and file storage out of the box, Supabase acts as a force multiplier, allowing
development efforts to be focused on creating unique product value in the application
layer.24
The setup process is straightforward:
1.​ Project Creation: A new project is created through the Supabase Dashboard,

selecting a region geographically close to the expected user base to minimize
latency.24

2.​ Environment Variables: The dashboard provides a Project URL and an anon

(anonymous) key. These credentials must be stored securely in a .env.local file at
the root of the Next.js project. It is critically important to add .env.local to the
.gitignore file to prevent these secrets from ever being committed to version
control. Publicly exposing server-side keys can lead to catastrophic security
breaches and data loss, a painful lesson highlighted in developer communities.26
3.​ Schema Initialization: The SQL script defined in the previous section can be
executed directly within the Supabase SQL Editor to create the necessary tables
and relationships.24
4.​ Row Level Security (RLS): Implementing RLS is a non-negotiable step for
building a secure, multi-tenant CRM. RLS is a powerful PostgreSQL feature that
Supabase makes exceptionally accessible. It moves security and data access
logic from the application layer directly into the database itself, creating a
secure-by-default architecture.27 Instead of writing complex and error-prone
authorization checks in every API endpoint, simple SQL policies can enforce data
access rules at the source. For a CRM, a fundamental policy would be to ensure
users can only access their own data. This is achieved with a concise policy
statement:​
CREATE POLICY "Users can only see their own contacts." ON contacts FOR
SELECT USING (auth.uid() = owner_id);.28 This single line of SQL provides
enterprise-grade security that is automatically enforced for every query made to
the​
contacts table. This approach represents a profound simplification and security
enhancement, and it should be considered the correct and mandatory way to
build a secure application on this stack.

C. Mastering Drizzle ORM: Type-Safe SQL in TypeScript
Drizzle ORM serves as the type-safe bridge between the Next.js application code and
the Supabase PostgreSQL database. It occupies a strategic sweet spot for a
semi-technical founder. Unlike some traditional Object-Relational Mappers (ORMs)
that heavily abstract SQL away, Drizzle embraces SQL's power and expressiveness
while wrapping it in a layer of end-to-end type safety.29 This means developers get full
IntelliSense autocompletion and, more importantly, compile-time error checking on
their database queries. This development pattern eliminates a vast category of
common runtime bugs and dramatically accelerates the development feedback loop.
The implementation workflow for Drizzle is as follows:

1.​ Installation: The necessary packages are installed via npm: npm install

drizzle-orm pg for the core library and the standard node-postgres driver, and
npm install -D drizzle-kit for the migration tooling.30
2.​ Schema Definition: A src/db/schema.ts file is created. In this file, the database
tables are defined using Drizzle's declarative syntax (e.g., pgTable, serial, text,
timestamp). This TypeScript code serves as the single source of truth for the
database schema, and it is designed to mirror the SQL schema defined earlier.31
3.​ Configuration: A drizzle.config.ts file is created at the project root. This file
configures Drizzle Kit, telling it where to find the schema file and where to output
migration files. It also contains the database connection string. For connecting to
Supabase, it is essential to use the Connection Pooler URL provided in the
Supabase database settings. This ensures that database connections are
managed efficiently, preventing the application from exhausting the limited
number of direct connections available, which is crucial for performance and
stability in a serverless environment.31
4.​ Migration Workflow: Managing schema changes is a disciplined, two-step
process. First, after modifying the schema.ts file, the command npx drizzle-kit
generate is run. This command compares the TypeScript schema with the last
known state of the database and generates a new SQL migration file containing
the necessary ALTER TABLE, CREATE TABLE, or other DDL statements. Second,
the command npx drizzle-kit migrate (or supabase db push for those using the
Supabase CLI) is executed to apply this newly generated SQL file to the live
database, bringing its structure in sync with the code.29 This repeatable and
version-controlled process for schema management is a cornerstone of
professional software development.

III. The Interaction Layer: Building with Next.js 19 and Server
Actions
With the data foundation established, the next step is to build the application logic
that connects the database to the user interface. The modern Next.js stack, with its
App Router and Server Actions, provides a highly performant and developer-friendly
environment for creating this interaction layer.

A. Structuring Your Next.js Application with the App Router

The Next.js App Router, introduced in version 13 and refined since, represents a
paradigm shift in how React applications are structured and rendered. It employs a
file-system-based routing system that is both intuitive and powerful, guiding
developers toward building more performant applications by default.34
The core conventions are straightforward:
●​ Folders within the app/ directory define URL segments (e.g.,

app/dashboard/contacts/ maps to /dashboard/contacts).
●​ A page.tsx file within a folder defines the unique UI for that route.
●​ A layout.tsx file defines a shared UI shell (like a header, sidebar, and footer) that
wraps child pages and layouts, preserving state during navigation.34
●​ Special files like loading.tsx and error.tsx provide built-in support for handling UI
states gracefully. loading.tsx automatically wraps a page in a React Suspense
boundary, showing a loading indicator while data is being fetched. error.tsx
creates an error boundary, preventing a component-level error from crashing the
entire application and allowing for a graceful recovery.34
The most significant architectural concept within the App Router is the distinction
between Server Components and Client Components.35
●​ Server Components are the default in the App Router. They run exclusively on

the server and are never included in the client-side JavaScript bundle. This makes
them ideal for tasks that require direct access to backend resources, such as
fetching data from a database or accessing secret environment variables like API
keys. Because their code doesn't ship to the browser, they help keep the
client-side bundle size small, leading to faster initial page loads. The primary data
display components of the CRM will be Server Components.
●​ Client Components are explicitly opted into by placing the 'use client' directive
at the top of a file. While they are still pre-rendered on the server for the initial
HTML load (Server-Side Rendering), their code is also sent to the browser to be
"hydrated." Hydration is the process of attaching event listeners and making the
component interactive. Client Components are necessary only when using
client-side hooks like useState for managing state, useEffect for lifecycle effects,
or event handlers like onClick.35
This architecture leads to "performance by default." By prioritizing server-side
rendering and minimizing the amount of JavaScript sent to the client, the App Router
helps create applications that feel incredibly fast and responsive. For a data-intensive

application like a CRM, where users expect instant access to information, this
performance benefit is not merely a technical detail; it is a core feature that directly
enhances user satisfaction and productivity.36

B. The Power of Server Actions for Data Mutations
Server Actions are the modern, integrated solution for handling data mutations
(Create, Update, Delete operations) in Next.js applications. They are asynchronous
functions that are defined on the server but can be invoked directly from client
components. This architecture completely eliminates the need for manually creating
and fetching from traditional API routes for these common operations, streamlining
the development process significantly.35
The workflow for implementing Server Actions is highly efficient:
1.​ A dedicated file, such as lib/actions.ts, is created to house the mutation logic.
2.​ The 'use server' directive is placed at the top of this file, marking all its exported

functions as Server Actions.
3.​ Mutation functions are written as standard asynchronous JavaScript functions,
for example: export async function createContact(formData: FormData) {... }.
Inside these functions, the Drizzle ORM instance is used to perform the necessary
database operations (e.g., db.insert(...)).33
4.​ In a React form component, this Server Action can be passed directly to the
action prop of a <form> element: <form action={createContact}>. When the form
is submitted, Next.js handles the RPC (Remote Procedure Call) to the server,
executes the function, and returns the result.
5.​ To ensure the UI reflects the data changes, Server Actions provide a mechanism
for on-demand cache revalidation. After a successful database mutation, calling
the revalidatePath('/dashboard/contacts') function inside the action tells Next.js
to purge its server-side cache for that specific page. The next time the user visits
that page, Next.js will refetch the data, ensuring the UI is always up-to-date with
the latest information from the database.37
This approach marks the end of API boilerplate for many common use cases. In the
older pages router paradigm, adding a new contact would require creating and wiring
together at least five distinct pieces of code: the form component itself, a client-side
fetch call to an API endpoint, a separate API route file (e.g., pages/api/contacts.ts), the
server-side handler logic within that file, and often separate TypeScript type
definitions for the API request and response payloads. This process is tedious,

verbose, and prone to error.
Server Actions, when combined with the type safety of Drizzle ORM, collapse this
entire workflow into a single, cohesive unit. A developer defines the database schema
in TypeScript, writes a type-safe Server Action to modify that schema, and calls that
action directly from the UI. The type information flows seamlessly from the database
definition all the way to the form component, providing autocompletion and
compile-time checks throughout. This represents a significant reduction in boilerplate
code, which translates directly into faster, more reliable product development.

IV. The Component Philosophy: Mastering shadcn/ui and Tailwind
4
Crafting a user interface that is beautiful, functional, and maintainable is paramount to
the success of a modern application. The chosen stack of shadcn/ui and Tailwind CSS
4 is an excellent combination that prioritizes developer control, customization, and
long-term maintainability over the rigid constraints of traditional component libraries.

A. Why shadcn/ui is Not a Component Library (And Why That's a Good Thing)
It is crucial to understand the core philosophy of shadcn/ui: it is not a conventional
component library delivered as an opaque package in node_modules. Instead, it is a
collection of reusable components whose source code is copied directly into the
project's codebase via a CLI command.38 When a developer runs
npx shadcn-ui@latest add button, the actual button.tsx file is placed into their
components/ui directory.
This approach provides several strategic advantages:
●​ Full Ownership and Control: The developer owns the code. They are free to

modify any aspect of the component—its structure, its styling, its behavior—to
perfectly match the application's specific needs. This avoids the "black box"
problem common with traditional libraries, where developers must fight against
the library's default styles and behaviors or write complex overrides to achieve
their desired design.
●​ Transparency and Learnability: Because the source code is present and
readable, developers can see exactly how each component is built. This provides

a valuable learning opportunity and makes debugging far simpler than trying to
diagnose issues within a compiled, third-party package.
●​ AI-Readiness: This open-code philosophy makes the codebase inherently
"AI-Ready." Modern large language models (LLMs) and AI coding assistants can
read, understand, and even suggest modifications or improvements to the
component code because it is part of the local project context. This is not
possible with pre-packaged libraries.38
Getting started is a simple, two-step process. First, npx shadcn-ui@latest init is run to
set up the project with necessary dependencies and configuration files (like
tailwind.config.ts and components.json). From then on, components are added as
needed with the add command.39

B. Styling with Tailwind CSS 4: A New Era of Performance
Tailwind CSS operates on a "utility-first" principle. Instead of writing custom CSS
classes like .card or .primary-button, developers build designs by composing small,
single-purpose utility classes directly in their HTML (or JSX) markup. Classes like flex,
pt-4 (padding-top), text-center, and bg-blue-500 can be combined to create any
design imaginable.41
The upcoming Tailwind CSS v4 represents a ground-up rewrite of the framework,
optimized for the modern web and focused on two key areas that directly benefit a
founder:
1.​ Unprecedented Performance: The new engine promises significantly faster

build times—up to 5x faster for full builds and over 100x faster for incremental
builds. This means less time waiting for the development server to compile
changes and more time spent building the product.44
2.​ Radical Simplicity: The configuration process has been streamlined. In many
cases, all that is required is to install the package and add a single line—@import
"tailwindcss";—to the global CSS file. The framework is smarter, automatically
detecting template files and using modern CSS features like cascade layers to
manage styles efficiently with less boilerplate.44
The combination of Tailwind's constrained design tokens (the predefined, harmonious
scale for spacing, colors, typography, etc.) and shadcn/ui's well-architected
components allows a solo founder to build a professional-grade design system
without needing a dedicated designer. The framework itself enforces the rules of

good design—consistent spacing, a cohesive color palette, and proper typographic
hierarchy. This prevents the common early-stage product pitfall of an inconsistent,
"Frankenstein" UI where every element is slightly different. The result is an application
that looks and feels like it was created by a much larger, more mature organization,
which is essential for building user trust and credibility.41

C. Building the Core CRM UI: From Dashboards to Data Tables
With the component philosophy and styling engine in place, the next step is to
construct the core UI elements of the CRM. The examples provided by shadcn/ui itself
serve as excellent inspiration and a practical starting point for this process.45
The implementation will focus on applying modern CRM UI/UX best practices, which
prioritize efficiency, clarity, and minimizing the user's cognitive load. This means
establishing a clear visual hierarchy, designing seamless workflows, and reducing the
number of clicks required to perform common, high-frequency tasks.2
Key UI components will be built using shadcn/ui's primitives:
●​ The Main Dashboard: The central hub of the application will be constructed

using a grid of Card components. Each card will be a modular unit designed to
surface a key piece of information or an AI-driven insight, such as "Top Priority
Leads," "Recent Activity," or "Deals at Risk." This modularity allows for a dense yet
scannable interface.
●​ Data Display and Interaction: For displaying lists of contacts, companies, and
deals, the Table component will be implemented. This component provides a solid
foundation for features like sorting, filtering, and pagination. When a user needs
to view or edit a specific record, a Sheet (a side panel) or a Dialog (a modal) can
be used to present the detail view without requiring a full page navigation,
keeping the user in context.
●​ User Input and Forms: For creating new records (e.g., "Add New Contact"), the
Dialog component will host a form. The shadcn/ui Form component is particularly
powerful as it is built to integrate seamlessly with popular form management
libraries like react-hook-form and schema validation libraries like zod. This
combination enables the creation of robust forms with real-time, client-side
validation, providing a superior user experience for data entry.

V. The Intelligence Engine: Integrating AI to Fulfill the Vision
This section details the implementation of the "AI partner," the core differentiator of
the CRM. By integrating advanced AI capabilities using the Vercel AI SDK, the
application will transform from a simple data management tool into an intelligent
system that actively assists the user in their work.

A. Architecture for AI: The Vercel AI SDK
The Vercel AI SDK is a TypeScript-first toolkit designed by the creators of Next.js to
simplify the process of building AI-powered applications. It provides a unified,
provider-agnostic API that abstracts away the complexities of interacting with various
large language models (LLMs) from providers like OpenAI, Anthropic, and Google. The
SDK is engineered to handle the most challenging aspects of building AI interfaces,
such as streaming responses, managing conversational state, and enabling LLMs to
interact with external tools.49
The setup and integration process is designed to be seamless within a Next.js
application:
1.​ Installation and Configuration: The necessary packages, ai and

@ai-sdk/openai, are installed. The developer's OpenAI API key is then stored
securely in the .env.local file under the key OPENAI_API_KEY, which the SDK
automatically detects.11
2.​ Backend Route Handler: A Next.js Route Handler is created at
app/api/chat/route.ts. This server-side endpoint acts as a secure proxy to the
OpenAI API. It receives requests from the frontend, uses the streamText function
from the AI SDK to call the LLM with the user's prompt, and then streams the
response back to the client.49
3.​ Frontend Hook: In the client-side React components, the useChat hook is used.
This simple hook abstracts away all the complexity of managing the state of a
conversation. It provides the current list of messages, the user's current input, a
handleSubmit function for the form, and a handleInputChange function for the
input field, making it trivial to build a fully functional chat interface.49
A critical feature enabled by the Vercel AI SDK is response streaming. When an LLM
generates a response, it can take several seconds for the full text to be produced.
Waiting for this entire process to complete before displaying anything to the user
creates a sluggish and frustrating user experience. Streaming solves this by displaying
the response token-by-token as it is generated by the model. This makes the

application feel alive, interactive, and instantly responsive. For a product centered on
an "AI partner," this real-time interaction is fundamental to the user's perception of
the AI's intelligence and responsiveness. The Vercel AI SDK makes implementing this
complex pattern a trivial matter, and it is a key detail that will set the product's user
experience apart from competitors.51

B. Implementing the Analyst: A Natural Language Interface to Your CRM
The "Analyst" pillar of the intelligence engine is the Natural Language Interface to the
Database (NLIDB), which allows users to query their data using plain English. The core
challenge is to safely and reliably translate a user's query (e.g., "Show me my biggest
deals expected to close this month") into an accurate and executable SQL query.14
The modern solution to this problem is an "agentic loop," which leverages the reasoning
capabilities of LLMs and the tool-calling functionality of the Vercel AI SDK. This pattern is far
more robust than simple text-to-SQL translation.
The architecture of this agentic loop is as follows:
1.​ The user submits their natural language query through the UI, which is managed

by the useChat hook.
2.​ The query is sent from the client to the /api/chat route handler on the server.
3.​ Inside the route handler, the LLM is invoked using the streamText function.
Crucially, two additional pieces of information are provided to the model:
○​ Schema Context: A textual representation of the relevant database schema
(table names, column names, and their relationships), which can be
programmatically generated from the Drizzle schema files. This tells the LLM
what data is available.
○​ Tool Definition: A tool named executeQuery is defined using the AI SDK's
tool helper. This tool has a description ("Executes a SQL query against the
CRM database") and a defined input schema (a string for the SQL query).51
4.​ The LLM analyzes the user's query, understands their intent, and sees that it has
the executeQuery tool available. It reasons that to answer the user's question, it
needs to query the database. It then decides to call the executeQuery tool,
generating the appropriate SQL string to pass as an argument.
5.​ The server-side execute function, which was defined as part of the tool, receives
the SQL string proposed by the LLM. At this stage, it is imperative to validate
and sanitize this query to prevent SQL injection attacks.
6.​ The validated query is then safely executed against the Supabase database using
the Drizzle instance.
7.​ The JSON result set from the database query is returned to the LLM as the

output of the tool call.
8.​ The LLM now has the raw data it needs. It proceeds to the final step of the plan:
synthesizing this data into a human-readable, natural language summary and
streaming that final answer back to the user's browser.
This pattern is exceptionally powerful because it uses the LLM for its greatest
strength: reasoning and planning. The LLM doesn't just blindly translate text to SQL; it
forms a multi-step plan ("First, I need to get the data by calling the executeQuery tool.
Then, I will summarize the results for the user."). The application code remains in full
control of the actual execution, providing a blend of AI-driven flexibility and
developer-controlled security that was previously very difficult to achieve. This is the
most direct and modern architecture for building the NLIDB feature.18

C. Implementing the Oracle: Predictive Lead Scoring
The "Oracle" pillar is the Predictive Lead Scoring feature, which uses historical sales
data to rank new leads and help the sales team prioritize their efforts.8 A key
prerequisite for this feature is data. From its first day of use, the CRM must be
structured to capture a sufficient volume of historical data with clear, binary outcomes
(e.g., a minimum of 40 deals marked as "Closed Won" and 40 marked as "Closed
Lost") to serve as a training set for any predictive model.8
For a founder building an initial version, a practical implementation strategy can
leverage an LLM directly, bypassing the need for a complex, custom machine learning
pipeline:
1.​ Feature Engineering: The process begins by identifying the key attributes

(features) that are likely to correlate with a successful sale. This includes
demographic or firmographic data from the contacts and companies tables (e.g.,
job_title, company_size, industry) and behavioral data captured in the interactions
table (e.g., number_of_emails, attended_webinar, visited_pricing_page).10
2.​ LLM as a Zero-Shot Scorer: A server-side function is created that takes the
engineered features for a new lead. This data is then formatted into a carefully
crafted prompt and sent to an LLM. The prompt should include:
○​ A clear description of the task: "You are an expert sales analyst. Your task is to
score the following lead on a scale of 1 to 100 based on their likelihood to
become a customer."
○​ Context about the ideal customer profile.
○​ A few examples of both high-quality (won) and low-quality (lost) leads from

the historical data (a technique known as few-shot prompting).
○​ The data for the new lead to be scored.
○​ A crucial final instruction: "Provide a numerical score and a brief, bulleted list
of the top three reasons for your score."
The most important aspect of the user experience for this feature is explainability. A
black-box score of "87" is opaque, intimidating, and not particularly actionable.
However, a score of "87" accompanied by a clear explanation—such as "This lead is a
VP of Engineering at a 500-person tech company (ideal customer profile) and they
have viewed the pricing page twice this week"—is incredibly powerful. It builds user
trust, provides valuable context, and transforms the feature from a simple number into
a genuine piece of actionable intelligence. Users are far more likely to trust and act
upon AI recommendations if they understand the underlying reasoning. While
traditional predictive models often lack this transparency, LLMs excel at generating
natural language explanations. By prompting the model to produce both the score
and the reasoning, the feature becomes not only predictive but also transparent and
educational, reinforcing the core concept of the "AI partner".9

VI. The Full Loop: Deployment, Feedback, and Continual Learning
A product's journey does not end when the code is written. It truly begins when the
application is in the hands of users, and a robust system is in place to gather
feedback and drive continuous improvement. This final section covers the seamless
deployment of the application and the establishment of the feedback loops necessary
to fulfill the vision of "continual learning."

A. Deploying to Vercel: From Localhost to Production
The entire technology stack—Next.js, Supabase, and Drizzle—is optimized for a
modern, streamlined deployment workflow on Vercel. Vercel, as the creator of Next.js,
provides a "zero-configuration" deployment experience that allows a founder to focus
on product development rather than cloud infrastructure management.
The deployment process is a straightforward checklist:
1.​ The application's code is pushed to a Git repository (e.g., GitHub, GitLab, or

Bitbucket).
2.​ This repository is then imported into a new Vercel project through the Vercel

dashboard.54
3.​ The environment variables required for production must be configured in the
Vercel project settings. This is where the Supabase keys
(NEXT_PUBLIC_SUPABASE_URL, NEXT_PUBLIC_SUPABASE_ANON_KEY,
DATABASE_URL) and the OPENAI_API_KEY are securely stored. They should never
be hardcoded in the application source code.54
4.​ A push to the main branch of the Git repository automatically triggers a new
production deployment. Vercel's build system detects the Next.js framework and
handles the entire build, optimization, and deployment process without requiring
manual configuration.
The official Supabase integration on the Vercel Marketplace can further simplify this
process by automating the synchronization of environment variables between the two
platforms, creating a truly seamless DevOps experience.55

B. Establishing the Feedback Loop: The Engine for Learning
The product's capacity for "continual learning" is entirely dependent on the quality
and consistency of the feedback it receives from users. A multi-faceted approach to
gathering both qualitative and quantitative data is essential.
Recommended tools and strategies include:
●​ Visual Feedback and Bug Reporting: Integrating a tool like Userback or

Usersnap is highly effective. These tools add a simple, unobtrusive feedback
widget to the application. Users can click this widget to capture a screenshot of
their current view, annotate it with drawings and text to highlight issues or
suggestions, and submit a report with all the necessary technical context
(browser, OS, screen size) automatically included. This rich, visual feedback is far
more valuable for debugging and understanding user issues than a simple text
description in an email.56
●​ In-Context Micro-Surveys: A tool like Hotjar allows for the creation of short,
targeted surveys that appear at the most relevant moment in the user's journey.
For example, immediately after the AI partner generates a draft email, a small
pop-up can ask, "How helpful was this suggestion?" with a simple 1-5 star rating.
This provides direct, quantitative feedback on the performance and utility of
specific AI features, allowing for data-driven prioritization of improvements.59
●​ Session Replays: To understand user behavior at scale, tools like Hotjar or
FullStory are invaluable. These platforms record anonymized user sessions,

allowing the development team to watch video-like replays of how real users
interact with the CRM. This is the most effective way to identify points of friction,
discover where users get stuck or confused, and see which features are being
used most frequently. This qualitative insight is crucial for empathizing with the
user and improving the overall user experience.58

C. Iterating with Confidence: A/B Testing AI Features
The ultimate test of any new feature, especially an AI-powered one, is whether it
measurably improves business outcomes. A/B testing provides a scientific method for
answering this question. However, testing AI features differs from traditional UI A/B
testing and requires a specific set of best practices.
The core objective is to scientifically prove that an updated AI model or a new AI
feature is actually making users more successful.
●​ Define a Business KPI, Not a Model Metric: The goal of an A/B test should

never be to improve a technical metric like "model accuracy" in isolation. The goal
must be tied to a key business metric. For the predictive lead scoring feature, the
primary Key Performance Indicator (KPI) to measure is the "lead-to-deal
conversion rate." For the generative email-drafting feature, the KPI might be
"average time to send a follow-up email" or "reply rate on first follow-up".60
●​ Establish a Clear Control Group: To test a new lead scoring model (Version B,
the challenger), it must be compared against a clear baseline (Version A, the
control). The control group might see scores from the old model, or perhaps no
scores at all. This allows for a direct comparison of user behavior and outcomes
between the two groups.62
●​ Isolate the Algorithmic Variable: The user interface and overall experience must
be identical for both the control and experimental groups. The only difference
should be the underlying AI model that is providing the data or suggestion. This
ensures that any observed difference in the KPI is due to the change in the AI, not
a change in the UI.60
●​ Measure for Statistical Significance: The experiment must run for a sufficient
duration to collect enough data to be confident that any observed change in the
KPI is a real effect and not simply due to random chance.
This mindset—measuring business impact, not just technical performance—is critical
for a product-focused founder. A lead scoring model that is technically 99% accurate
according to offline tests but is ignored by users in practice is a failure. Conversely, a

model that is only 85% accurate but is trusted and acted upon by users, leading to a
measurable 10% lift in their conversion rate, is a massive success. A/B testing is the
essential bridge between offline model evaluation and real-world business value. It is
the ultimate tool for ensuring that the product's "continual learning" is driving
meaningful and sustainable growth.63
Works cited
1.​ CRM Design Best Practices - Adam Fard UX Studio, accessed August 14, 2025,

https://adamfard.com/blog/crm-design

2.​ Top 10 CRM Design Best Practices for Success - Aufait UX, accessed August 14,

2025, https://www.aufaitux.com/blog/crm-ux-design-best-practices/

3.​ 10 Best AI CRMs to Enhance Your Sales and Services | Creatio, accessed August

14, 2025, https://www.creatio.com/glossary/ai-crm

4.​ AI in CRM: 16 Use Cases, Best Platforms, and Adoption Guidelines, accessed

August 14, 2025, https://www.itransition.com/ai/crm

5.​ Sales AI | AI-Powered Workflows for Customers | Outreach, accessed August 14,

2025, https://www.outreach.io/platform/sales-ai
6.​ AI Sales Automation: How It Can Transform - Master of Code Global, accessed
August 14, 2025,
https://masterofcode.com/blog/how-ai-can-automate-your-sales
7.​ The Future of Sales: How AI and Automation Are Transforming Go-to-Market
Strategies, accessed August 14, 2025,
https://business.columbia.edu/insights/ai-automation-transforming-go-to-market
-strategies
8.​ Configure predictive lead scoring | Microsoft Learn, accessed August 14, 2025,
https://learn.microsoft.com/en-us/dynamics365/sales/configure-predictive-lead-s
coring
9.​ What is Predictive Lead Scoring? - Clay, accessed August 14, 2025,
https://www.clay.com/glossary/predictive-lead-scoring
10.​Predictive Lead Scoring: What Does It Do & How Can You Use It in Your Analytics?,
accessed August 14, 2025,
https://www.activecampaign.com/blog/predictive-lead-scoring
11.​ Modern AI Integration: OpenAI API in Your Next.js App | by Adhithi Ravichandran |
Medium, accessed August 14, 2025,
https://adhithiravi.medium.com/modern-ai-integration-openai-api-in-your-next-j
s-app-f3a3ce2decf0
12.​NLI4DB: A Systematic Review of Natural Language Interfaces for Databases arXiv, accessed August 14, 2025, https://arxiv.org/abs/2503.02435
13.​arxiv.org, accessed August 14, 2025, https://arxiv.org/html/2503.02435v1
14.​Natural Language Interfaces for Tabular Data Querying and Visualization: A
Survey - arXiv, accessed August 14, 2025, https://arxiv.org/html/2310.17894v3
15.​Natural Language Query Engine for Relational Databases using Generative AI arXiv, accessed August 14, 2025, https://arxiv.org/html/2410.07144v1

16.​Natural Language Interface to Database Approach in the Task of Relational

Databases Design - CEUR-WS.org, accessed August 14, 2025,
https://ceur-ws.org/Vol-3373/paper20.pdf
17.​Natural language Interface for Database: A Brief review - ResearchGate,
accessed August 14, 2025,
https://www.researchgate.net/publication/266863909_Natural_language_Interfac
e_for_Database_A_Brief_review
18.​Neural Approaches for Natural Language Interfaces to Databases: A Survey,
accessed August 14, 2025, https://aclanthology.org/2020.coling-main.34/
19.​Natural Language Interfaces to Data - Now Publishers, accessed August 14, 2025,
https://www.nowpublishers.com/article/DownloadSummary/DBS-078
20.​A Survey on Natural Language Interface to Database - International Journal of
New Technologies in Science and Engineering, accessed August 14, 2025,
http://ijntse.com/upload/1542531999final%20paper.pdf
21.​CRM Database Schema Example (A Practical Guide) - Dragonfly, accessed
August 14, 2025, https://www.dragonflydb.io/databases/schema/crm
22.​crm/crm-core/src/main/resources/schema/crm-postgresql-schema.sql at master
- GitHub, accessed August 14, 2025,
https://github.com/jfifield/crm/blob/master/crm-core/src/main/resources/schema/
crm-postgresql-schema.sql
23.​A CRM Database: The Tables & Fields Behind the UI, accessed August 14, 2025,
https://crmswitch.com/crm/crm-database/
24.​Build a User Management App with Next.js | Supabase Docs, accessed August 14,
2025, https://supabase.com/docs/guides/getting-started/tutorials/with-nextjs
25.​Use Supabase Auth with Next.js, accessed August 14, 2025,
https://supabase.com/docs/guides/auth/quickstarts/nextjs
26.​How to add Drizzle to your Next.js and/or Supabase project - YouTube, accessed
August 14, 2025, https://www.youtube.com/watch?v=o0bAZdag_7Y
27.​Documentation: 17: 5.10. Schemas - PostgreSQL, accessed August 14, 2025,
https://www.postgresql.org/docs/current/ddl-schemas.html
28.​Complete Supabase Auth in Next.js 15 | OAuth Login | Forget & Reset Password YouTube, accessed August 14, 2025,
https://www.youtube.com/watch?v=D3HC_NyrTe8
29.​Drizzle ORM crash course : r/node - Reddit, accessed August 14, 2025,
https://www.reddit.com/r/node/comments/1aq2d85/drizzle_orm_crash_course/
30.​PostgreSQL - Drizzle ORM, accessed August 14, 2025,
https://orm.drizzle.team/docs/get-started-postgresql
31.​Drizzle with Supabase Database - Drizzle ORM, accessed August 14, 2025,
https://orm.drizzle.team/docs/tutorials/drizzle-with-supabase
32.​How to Create a Blog with Next.js, Supabase, and Drizzle ORM | by Turingvang Medium, accessed August 14, 2025,
https://medium.com/@turingvang/how-to-create-a-blog-with-next-js-supabaseand-drizzle-orm-45d5444947ba
33.​Next.js 15 + Drizzle ORM: A Beginner's Guide to CRUD Operations ..., accessed
August 14, 2025,

https://medium.com/@aslandjc7/next-js-15-drizzle-orm-a-beginners-guide-to-cr
ud-operations-ae7f2701a8c3
34.​Ultimate Guide to Routing with the App Router in Next.js 14 | by Ankit singh Medium, accessed August 14, 2025,
https://medium.com/@ankitsingh80741/ultimate-guide-to-routing-with-the-app-r
outer-in-next-js-14-0a19ecb67150
35.​App Router: Getting Started - Next.js, accessed August 14, 2025,
https://nextjs.org/docs/app/getting-started
36.​App Router | Next.js, accessed August 14, 2025,
https://nextjs.org/learn/dashboard-app
37.​How to refresh your data using Server Actions and Drizzle in Next.js - Sabin
Hertanu, accessed August 14, 2025,
https://sabin.dev/blog/how-to-refresh-your-data-using-server-actions-and-drizzl
e-in-nextjs
38.​Introduction - Shadcn UI, accessed August 14, 2025, https://ui.shadcn.com/docs
39.​Installation - Shadcn UI, accessed August 14, 2025,
https://ui.shadcn.com/docs/installation
40.​Manual Installation - Shadcn UI, accessed August 14, 2025,
https://ui.shadcn.com/docs/installation/manual
41.​Tailwind CSS: The Utility-First Revolution in Frontend Development - DEV
Community, accessed August 14, 2025,
https://dev.to/mikevarenek/tailwind-css-the-utility-first-revolution-in-frontend-de
velopment-3kk2
42.​A Primer on Tailwind CSS: Pros, Cons & Real-World Use Cases - Telerik.com,
accessed August 14, 2025,
https://www.telerik.com/blogs/primer-tailwind-css-pros-cons-real-world-use-ca
ses
43.​Tailwind CSS - Rapidly build modern websites without ever leaving your HTML.,
accessed August 14, 2025, https://tailwindcss.com/
44.​Tailwind CSS v4.0 - Tailwind CSS, accessed August 14, 2025,
https://tailwindcss.com/blog/tailwindcss-v4
45.​The Foundation for your Design System - shadcn/ui, accessed August 14, 2025,
https://ui.shadcn.com/
46.​Examples - Shadcn UI, accessed August 14, 2025,
https://ui.shadcn.com/examples/dashboard
47.​Shadcn UI Complete Guide: From Beginner to Pro in One Tutorial - YouTube,
accessed August 14, 2025, https://www.youtube.com/watch?v=urlCrgNO0HY
48.​Top CRM UI/UX Design Examples in Enterprise Applications - Coders.dev,
accessed August 14, 2025,
https://www.coders.dev/blog/great-examples-of-enterprise-applications-crm-uiux-design-patterns.html
49.​vercel/ai: The AI Toolkit for TypeScript. From the creators of Next.js, the AI SDK is a
free open-source library for building AI-powered applications and agents GitHub, accessed August 14, 2025, https://github.com/vercel/ai
50.​AI SDK - Vercel, accessed August 14, 2025, https://vercel.com/docs/ai-sdk

51.​Getting Started with Building AI Apps Using Vercel AI SDK - DEV Community,

accessed August 14, 2025,
https://dev.to/elfrontend/getting-started-with-building-ai-apps-using-vercel-ai-s
dk-2gn3
52.​Getting Started: Next.js App Router - AI SDK, accessed August 14, 2025,
https://ai-sdk.dev/docs/getting-started/nextjs-app-router
53.​A Complete Guide To Vercel's AI SDK // The ESSENTIAL Tool For Shipping AI Apps,
accessed August 14, 2025, https://www.youtube.com/watch?v=mojZpktAiYQ
54.​Deploy Next.js Supabase to Vercel - Makerkit, accessed August 14, 2025,
https://makerkit.dev/docs/next-supabase-turbo/going-to-production/vercel
55.​Vercel | Works With Supabase, accessed August 14, 2025,
https://supabase.com/partners/vercel
56.​Userback: Your #1 User Feedback Software, accessed August 14, 2025,
https://userback.io/
57.​31 Best Website Feedback Tools in 2025 - Usersnap, accessed August 14, 2025,
https://usersnap.com/blog/website-feedback-tool/
58.​23 Best User Feedback Tools to Improve Your Customer Experience | Userflow
Blog, accessed August 14, 2025,
https://www.userflow.com/blog/23-best-user-feedback-tools-improve-your-cust
omer-experience-with-ease
59.​How to use website feedback tools for improved UX, conversions, and business
growth, accessed August 14, 2025,
https://www.hotjar.com/blog/website-feedback-tools/
60.​A/B Testing — What it is, examples, and best practices - Adobe Experience Cloud,
accessed August 14, 2025,
https://business.adobe.com/blog/basics/learn-about-a-b-testing
61.​A/B Testing Best Practices Guide - Twilio Segment, accessed August 14, 2025,
https://segment.com/growth-center/a-b-testing-definition/best-practices/
62.​General guidance on conducting A/B experiments | Vertex AI Search for
commerce | Google Cloud, accessed August 14, 2025,
https://cloud.google.com/retail/docs/a-b-testing
63.​A/B experiments for AI applications - Azure AI Foundry | Microsoft ..., accessed
August 14, 2025,
https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/a-b-experimentatio
n

