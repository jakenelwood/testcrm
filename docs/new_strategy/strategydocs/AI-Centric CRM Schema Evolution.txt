From System of Record to System of Intelligence: An
Architectural Blueprint for Your AI-Centric CRM

Part I: The Bedrock - What the Database Design Gets Right and
Why It's Critical for AI
The provided database architecture document represents an exceptionally strong
foundation for a modern Customer Relationship Management (CRM) system.1 Its
adherence to classical database design principles is not merely "good practice"; it is
the essential, non-negotiable prerequisite for building a robust, scalable, and, most
importantly, trustworthy AI-centric product. The philosophy of "integrity by default,
performance by design" is the correct starting point, as high-quality AI is
fundamentally impossible without high-integrity data.1 This section affirms the core
tenets of that design and reframes them within the context of an AI-first strategy,
demonstrating how this solid bedrock directly enables the intelligence layer that will
be built upon it.

Affirming the "Integrity by Default" Philosophy
The central philosophy articulated in the design document—"integrity by default,
performance by design"—is the cornerstone of a successful data platform.1 In an AI
context, this principle evolves into "trustworthy by default, intelligent by design." An AI
system's outputs are only as reliable as its inputs. Flawed, inconsistent, or redundant
data will inevitably lead to a flawed, inconsistent, and unreliable AI. By prioritizing data
integrity from the outset, the proposed architecture ensures that the "System of
Record" is clean, canonical, and correct. This provides a stable launchpad for building
the "System of Intelligence." Many engineering teams, in a rush to build features,
bypass these foundational steps, only to find themselves mired in technical debt and
data quality issues that cripple their ability to innovate, particularly in the AI domain.
The proposed design avoids this common and often fatal pitfall.

The Power of a Normalized Core (Third Normal Form)
The recommendation to build the core transactional tables to at least Third Normal
Form (3NF) is a critical strategic decision.1 The document correctly identifies the
primary benefits of this approach for an Online Transaction Processing (OLTP)
system: guaranteed data integrity, the prevention of update and deletion anomalies,
and improved write performance for common CRM tasks like logging calls or updating
opportunities.1
Beyond these database-centric virtues, this normalized structure has profound
implications for an AI strategy. Normalization serves as the first and most effective line
of defense against training data contamination. When generating embeddings—the
numerical representations that allow an AI to understand the "meaning" of data—it is
imperative to embed the single, authoritative source of truth.
Consider a denormalized schema where a contact's email address and job title are
copied into various activities or opportunities records. If that contact's title changes,
the update might only be applied to the primary contacts table, leaving stale,
incorrect data littered throughout the system. When an AI is later tasked with finding
"similar deals" or "contacts with similar roles," it will be learning from a poisoned well
of conflicting information. Its understanding of a "contact" becomes fractured and
ambiguous. The 3NF approach, by ensuring that a fact like a contact's title exists in
exactly one place, guarantees that the AI is always trained on the canonical, most
up-to-date information. This structural integrity is not just a database concern; it is a
prerequisite for building a reliable and accurate AI model.

Strategic Denormalization via Materialized Views
The proposed hybrid architecture, which pairs a normalized core with denormalized
projections for analytics, is a sophisticated and highly effective solution.1 The use of
PostgreSQL's
MATERIALIZED VIEW feature to serve read-heavy Online Analytical Processing (OLAP)
workloads is a particularly astute choice.1 As the document notes, this allows sales
dashboards and reports to query pre-computed, pre-joined data, delivering results in
milliseconds without burdening the live transactional tables.1
This strategy offers a second, equally crucial benefit for an AI-centric platform: it
creates a clean and performant interface for bulk data operations required for AI
model development. In the future, the product will likely require exporting millions of

activities records, joined with their associated opportunities data and company
firmographics, to fine-tune a custom language model. Executing such a massive,
complex JOIN operation against the live, normalized OLTP tables during business
hours would be a performance disaster, potentially grinding the user-facing
application to a halt.
The MATERIALIZED VIEW acts as the perfect staging area for these workloads. A view
like sales_analytics_view can be refreshed nightly, creating a single, wide,
denormalized table optimized for bulk export. This effectively isolates the demanding
needs of the data science and machine learning lifecycle from the high-concurrency,
low-latency requirements of the end-user application. This separation ensures that
the system remains fast and responsive for sales reps while providing data scientists
with the efficient access they need. It is a forward-thinking design that anticipates the
needs of a mature AI strategy.

Why the Data Type Choices are Mission-Critical
The document's deliberate selection of specific PostgreSQL data types is another
hallmark of a professional-grade architecture.1 These choices are not arbitrary
preferences; they have direct impacts on scalability, integrity, and the ability to
implement advanced features.
●​ UUID vs. BIGSERIAL: The choice of UUID for primary keys is superior for a

modern, distributed application.1 As the design notes, UUIDs can be generated by
the application without a database round-trip, which is essential for architectures
involving microservices, offline clients, or parallel data ingestion pipelines.1 This
prevents ID collisions when merging data from disparate sources, a critical feature
for a system designed for future scale.
●​ TIMESTAMPTZ: Mandating TIMESTAMPTZ (timestamp with time zone) for all
temporal data is non-negotiable.1 It stores all timestamps in UTC and converts
them to the client's local time zone on retrieval, eliminating ambiguity.1 For an AI,
this is vital for establishing an unambiguous sequence of events. Understanding
causality—for example, "did the marketing email​
precede the customer call that led to a deal?"—is impossible without absolute,
timezone-aware timestamps.
●​ JSONB: The use of JSONB for custom fields is the ideal modern solution, vastly
superior to legacy approaches like EAV tables.1 Its true power in an AI context,
however, lies in its flexibility for rapid product iteration. As the AI generates new
insights—such as sentiment scores, keyword extractions, or automated

classification tags for an activity—these semi-structured outputs can be stored
directly in a​
JSONB column without requiring a disruptive schema migration (ALTER TABLE).
This allows the development team to experiment with and deploy new AI features
at a much faster pace.
●​ NUMERIC: The insistence on using the NUMERIC type for all financial data is
fundamentally about correctness.1 Floating-point types (​
REAL) are subject to rounding errors that can accumulate, leading to inaccurate
financial reporting.1 For a system of record like a CRM, this is unacceptable. The​
NUMERIC type guarantees exact precision for monetary values, ensuring the
integrity of the most critical business metrics.

Part II: The Unfair Advantage - Architecting for Velocity with
Supabase and Drizzle

A startup's most valuable and non-renewable resource is time. The right technology
stack does not merely enable a product; it accelerates its development and iteration.
The choice of Supabase and the Drizzle ORM is a strategic decision to out-execute
competitors by maximizing developer velocity, reducing operational overhead, and
building on a foundation of type-safe confidence.

Why Supabase is More Than Just a Database
Supabase is not simply a managed PostgreSQL host; it is a comprehensive
backend-as-a-service platform built around a standard, open-source PostgreSQL
database.3 This approach provides the stability and power of PostgreSQL, as praised
in the initial design document, while abstracting away a significant amount of backend
development and operational complexity. By choosing Supabase, the project gains a
suite of essential, production-ready services out of the box, including authentication,
file storage, serverless edge functions, and automatically generated RESTful and
GraphQL APIs.5 This frees the development team from building and maintaining this
commodity infrastructure, allowing them to focus entirely on the unique,
value-creating features of the AI-centric CRM.
The most profound advantage of Supabase for this specific use case, however, is the

colocation of transactional data and vector embeddings. The platform's native
integration with the pgvector extension means that the vector embeddings—the
numerical representations of CRM data used for AI tasks—live in the same database,
and often in the same table, as the original source data.4
This is not a minor convenience; it is a fundamental architectural simplification that
eliminates a massive source of engineering pain. Numerous case studies highlight
companies migrating away from dedicated vector databases like Pinecone or
Weaviate to Supabase specifically to solve the challenges of keeping vector
embeddings and their associated metadata in sync.3
A competitor using a separate vector database must manage a distributed system
from day one. When a sales rep logs a call, the application must first save the call's
metadata (e.g., owner_id, company_id, deal_stage) to their primary PostgreSQL
database. Then, it must generate an embedding for the call notes and push that
vector to the separate vector database. To perform a filtered semantic search—such
as "find call notes that sound like this complaint, but only for enterprise customers in
the 'Finance' industry"—the application must execute a complex, multi-step query. It
first queries the vector database to get a list of semantically similar vector IDs, and
then takes that list of IDs back to the PostgreSQL database to filter them by the
required metadata. This approach is slow, complex, and brittle, as it relies on keeping
two separate data stores perfectly synchronized.
With Supabase and pgvector, this entire operation collapses into a single, atomic, and
highly performant SQL query. The ability to combine traditional WHERE clause filtering
with vector similarity search in one operation is a strategic superpower that
dramatically simplifies development and enables a new class of powerful AI features.

Why Drizzle ORM is a Developer Flywheel
Drizzle ORM is a modern, TypeScript-native Object-Relational Mapper that functions
more like a lightweight, schema-first query builder than a traditional, heavyweight
ORM.8 It is designed to work seamlessly with TypeScript to provide end-to-end type
safety, from the database schema definition to the application's query results.
The core workflow begins with defining the database schema in a TypeScript file,
schema.ts.10 Drizzle uses this single source of truth to automatically infer the
TypeScript types for every table and every possible query result. This creates a

powerful "developer flywheel" that drives development velocity and code quality.
The primary benefit is the ability to refactor with confidence. When a business
requirement necessitates a change to the database schema—for example, adding a
priority_score column to the opportunities table—the impact is immediately visible at
compile time. After the developer adds the new field to the Drizzle schema definition,
the TypeScript compiler acts as an automated assistant, highlighting every single file
in the codebase where the old Opportunity type is used and now needs to be updated
to account for the new field. It effectively generates a perfect to-do list of required
changes.
This process eliminates an entire class of common and difficult-to-diagnose runtime
errors that occur when application code falls out of sync with the database schema.
In a typical development environment without this level of type safety, a developer
might add the column to the database but inevitably miss updating one of the many
queries that select from that table. The bug might not surface until a specific,
rarely-used feature is triggered in production. With Drizzle, this scenario is impossible.
The code will not compile until every reference is correctly updated. This safety net
reduces the cognitive load on developers, minimizes the need for exhaustive manual
code searches, and dramatically cuts down on bugs, allowing a small team to build
and iterate on complex features with the speed and confidence of a much larger
organization.

Part III: The Intelligence Layer - A Founder's Guide to pgvector in
Supabase

The intelligence layer is what will transform the CRM from a passive system of record
into an active system of intelligence. This is powered by vector embeddings and
similarity search, a technology that allows the database to understand the semantic
meaning of data, not just its literal content.

What are Embeddings and Why Do They Matter for a CRM?

At its core, an embedding is a process that converts complex data, like a piece of text,
into a list of numbers called a "vector." This vector represents the data's position in a
high-dimensional "semantic space." A useful analogy is to imagine every call note,
email, and company description in the CRM as a point on a giant, multi-dimensional
map. Embeddings provide the GPS coordinates for each of these points. The key
property of this map is that points with similar meanings are located close to each
other, even if they use entirely different words.
This capability unlocks a new class of "superpowers" for the CRM, moving beyond
simple keyword search to true semantic understanding 3:
●​ Semantic Search: A user can search for "unhappy customer who mentioned

billing," and the system can find a call note that says, "client was frustrated with
the invoice." The system understands that "unhappy" and "frustrated" are
semantically similar concepts.
●​ Opportunity Matching: A sales manager can ask to "show me deals that are
similar to our biggest win this quarter." The system can analyze the descriptions,
call notes, and emails associated with the winning deal and find other in-progress
opportunities that share similar characteristics, customer needs, or pain points.
●​ Automated Clustering and Tagging: The system can automatically group leads
based on the needs described in their initial outreach emails or classify support
tickets by the underlying issue without relying on predefined tags, surfacing
emergent patterns in customer communication.

Step-by-Step Implementation Plan

Integrating this intelligence layer into the existing architecture is a straightforward
process with Supabase and pgvector.

1. Enable the vector Extension
The first step is to enable the pgvector extension within the Supabase project. This
can be done with a single click in the Supabase dashboard's "Extensions" section or
by running a simple SQL command in the SQL Editor.4

SQL

create extension if not exists vector;​

2. Modify the Schema to Store Embeddings
The next step is to decide where to store the vector embeddings. While creating a
separate document_embeddings table is a valid pattern for more complex scenarios,
the most direct and efficient approach for this CRM is to colocate the embedding
directly with its source text. This simplifies queries and eliminates the need for an
extra JOIN operation.
The recommendation is to start by adding an embedding column to the activities
table, as this table will contain the richest source of unstructured text (notes, emails,
etc.). This change is made in the Drizzle schema.ts file. A custom type helper is used
to define the vector type, specifying its dimensions, which will be determined by the
choice of embedding model.10

TypeScript

// In src/db/schema.ts​
import { pgTable, uuid, text, timestamp, specificType } from 'drizzle-orm/pg-core';​
import { activity_type_enum } from './custom-types'; // Assuming ENUMs are in a separate file​

​

// Drizzle helper to define the vector type​
const vector = (name: string, p: { dimension: number }) => specificType(name, {​
getSQLType: () => `vector(${p.dimension})`,​

});​
​
export const activities = pgTable('activities', {​
id: uuid('id').primaryKey().defaultRandom(),​
activity_type: activity_type_enum('activity_type').notNull(),​
subject: text('subject'),​
notes: text('notes'),​
activity_date: timestamp('activity_date', { withTimezone: true }).notNull(),​

//... other columns from the original design document​

​
// The new column for storing vector embeddings​
embedding: vector('embedding', { dimension: 384 }), // Dimension is a placeholder​

});​

3. Create the RPC Function for Similarity Search
The Supabase client libraries and auto-generated APIs connect to the database
through a middleware layer called PostgREST. This layer does not natively support the
custom operators provided by pgvector, such as the cosine distance operator <->.7 To
work around this, the similarity search query must be wrapped in a PostgreSQL
function. This function can then be called from the application via a Remote
Procedure Call (RPC), which PostgREST fully supports.7
The following SQL function takes a query embedding and returns the most similar
activities.

SQL

-- This function should be created in the Supabase SQL Editor​
create or replace function match_activities (​

query_embedding vector(384), -- Dimension must match the table column​
match_threshold float,​
match_count int​
)​
returns table (​
id uuid,​
subject text,​
notes text,​
similarity float​
)​
language sql stable​
as $$​
select​
a.id,​

a.subject,​
a.notes,​
1 - (a.embedding <=> query_embedding) as similarity​
from activities as a​
where 1 - (a.embedding <=> query_embedding) > match_threshold​
order by a.embedding <=> query_embedding​
limit match_count;​
$$;​

4. Create an Index for Performance

Without an index, a vector similarity search requires the database to perform a
"sequential scan"—comparing the query vector to every single vector in the activities
table.7 As the table grows to millions of rows, this operation becomes prohibitively
slow. A specialized vector index is therefore essential for production performance.
pgvector supports two main types of indexes: IVFFlat and HNSW.13 While IVFFlat
indexes are faster to build and consume less memory, HNSW (Hierarchical Navigable
Small World) indexes generally provide superior query speed and are more robust
against changing data, making them the recommended choice for user-facing,
real-time search features.13
The following SQL command creates an HNSW index on the embedding column, using
the cosine distance operator (vector_cosine_ops) as the distance metric, which is the
standard for text similarity tasks.

SQL

-- This index should be created after the column is added​
CREATE INDEX ON activities USING hnsw (embedding vector_cosine_ops);​

This index will dramatically accelerate similarity searches, allowing the system to find
the nearest neighbors in a massive dataset in milliseconds instead of minutes.

Part IV: The Embedding Model - A Pragmatic Decision Framework
for a Startup

The selection of an embedding model is a critical decision, but not for the reasons
many teams assume. The "best" model on a technical benchmark is often irrelevant.
The "right" model for an early-stage startup is the one that delivers the most business
value, for the least amount of effort, in the shortest amount of time. The decision
framework must prioritize implementation velocity and minimize operational drag, as
these are the factors that most directly impact a startup's ability to learn and find
product-market fit.

The Two Paths: API vs. Self-Hosted
There are two fundamental approaches to generating embeddings: using a
third-party API or hosting an open-source model.
●​ API-based Models (e.g., OpenAI, Cohere): These are offered as a managed

service. The application sends text to the provider's API and receives an
embedding vector in return.
○​ Pros: Zero operational overhead, no infrastructure to manage, pay-as-you-go
pricing, and access to state-of-the-art models.15
○​ Cons: Introduces network latency for each embedding generation, creates an
ongoing operational expense that scales with usage, and may raise data
privacy concerns for some enterprises.15
●​ Self-hosted Models (e.g., Sentence-Transformers): These are open-source
models that can be downloaded and run on a company's own infrastructure.
○​ Pros: No per-embedding cost (only infrastructure costs), data remains within
the company's private network, and offers complete control over the model
and its deployment.15
○​ Cons: Incurs significant operational complexity. It requires provisioning and
managing GPU servers, handling model deployment, ensuring high availability,
and scaling the infrastructure to meet demand. Cold start times can also be
an issue.16

The Startup's Dilemma and the Pragmatic Answer
An early-stage startup's primary goal is not to optimize costs or achieve a marginal
performance improvement on a benchmark; its primary goal is to learn. The faster the
team can ship features, gather user feedback, and iterate, the higher its probability of
success.
In this context, the choice of an embedding model becomes a proxy for the
company's strategic priority. A decision to self-host an open-source model is a
decision to prioritize long-term cost optimization and control over short-term speed.
For a pre-product-market-fit company, this is almost always the wrong trade-off. The
engineering time and focus required to build and maintain a production-grade model
hosting environment is a significant distraction from the core task of building the
product itself. A real-world case study highlights a team that benchmarked several
models and found that an open-source option was technically superior, yet they
pragmatically chose to stick with their existing API-based solution because the
engineering cost of switching was too high to justify the marginal benefit.16
Therefore, the only logical choice for an initial implementation is an API-based model.
It allows the team to integrate semantic search capabilities in a matter of hours, not
weeks, enabling the learning cycle to begin immediately. The cost of delay associated
with a self-hosting "science project" is far greater than the API costs that will be
incurred in the early stages.

The Recommended Model and Why
The recommended starting point is OpenAI's text-embedding-3-large model.17 This
recommendation is based on a pragmatic evaluation of performance, features, and
ease of use.
1.​ State-of-the-Art Performance: The text-embedding-3-large model

demonstrates top-tier performance on the MTEB (Massive Text Embedding
Benchmark), a standard for evaluating embedding quality. It shows a significant
improvement over its predecessor, ada-002, particularly in multilingual contexts,
which provides valuable future-proofing for the platform.18
2.​ The MRL Superpower: The most compelling feature of OpenAI's v3 models is
their support for Matryoshka Representation Learning (MRL).18 MRL is a technique
that encodes information at multiple levels of granularity within a single vector.
This means it is possible to generate the full, high-fidelity 3072-dimension
embedding but then​

truncate it to a smaller size (e.g., 1024, 512, or even 256 dimensions) before
storing and querying it. Remarkably, this truncation results in only a negligible loss
in accuracy while providing a massive reduction in storage costs and a significant
increase in query speed.18 For example, using a 512-dimension vector instead of a
3072-dimension one reduces storage and memory requirements by over 80%.
This feature provides the accuracy of a large model with the efficiency of a much
smaller one—the best of both worlds.
3.​ Developer Ecosystem: OpenAI's APIs are ubiquitous, exceptionally
well-documented, and familiar to the vast majority of developers. This reduces
friction for new hires and makes integration into the application straightforward.

The Decision Matrix
The following table summarizes the trade-offs for the most viable model options.

Factor

OpenAI
text-embedding-3-la
rge (Truncated)

Cohere
embed-english-v3.0

Sentence-Transforme
rs all-MiniLM-L6-v2

Implementation
Velocity

Highest. (Minutes to
implement API call)

High. (Minutes to
implement API call)

Lowest. (Weeks to
set up infra, deploy
model, manage
scaling)

Operational
Overhead

None. (It's an API)

None. (It's an API)

Very High. (The team
owns the entire
stack)

Performance
(MTEB)

64.6% 18

~63-64%
(Comparable to

~61.5% (Good, but
lower than latest

OpenAI) 21

APIs) 16

Low. ($0.0001 / 1k

High (in engineering
time and focus).

Cost at MVP Stage

Very Low. ($0.00013
/ 1k tokens)

19

tokens)

22

Cost at Scale

Medium-High. (API
costs will grow)

Medium-High. (API
costs will grow)

Low. (Only
infrastructure costs)

Key Feature

MRL (Truncation) 18

Strong RAG &

Total Control & Data

Recommendation

Start Here.

Multilingual Focus 23

Privacy 15

Strong Alternative.

Re-evaluate when
API costs exceed
$5k/month.

The action plan is clear: use text-embedding-3-large, truncate the embeddings to 512
dimensions for storage in pgvector, and proceed with building the product. This
choice should be revisited every six months to evaluate if the API costs have grown to
a point where the engineering investment in a self-hosted solution becomes
justifiable.

Part V: The Developer Flywheel - A Practical Workflow with
Drizzle and Supabase
This section demonstrates the end-to-end development workflow, tying together the
validated schema, the Supabase platform, the Drizzle ORM, and the pgvector
intelligence layer. This practical example illustrates how the chosen stack creates a
rapid, robust, and type-safe development loop, enabling a small team to build
sophisticated AI features with high velocity.
The workflow follows a complete "feature slice": adding the vector embedding
capability to the activities table.

1. Define the AI-Ready Schema in src/db/schema.ts
The process begins by updating the schema definition in Drizzle. Based on the
decision in Part IV, the activities table is modified to include an embedding column of
type vector(512) to store the truncated embeddings from OpenAI's
text-embedding-3-large model.

TypeScript

// src/db/schema.ts​
import { pgTable, uuid, text, timestamp, specificType } from 'drizzle-orm/pg-core';​

import { activity_type_enum } from './custom-types';​

​

// Drizzle helper to define the vector type​
const vector = (name: string, p: { dimension: number }) => specificType(name, {​
getSQLType: () => `vector(${p.dimension})`,​

});​
​
export const activities = pgTable('activities', {​
id: uuid('id').primaryKey().defaultRandom(),​
activity_type: activity_type_enum('activity_type').notNull(),​
subject: text('subject'),​
notes: text('notes'),​
activity_date: timestamp('activity_date', { withTimezone: true }).notNull(),​
created_at: timestamp('created_at', { withTimezone: true }).notNull().defaultNow(),​
updated_at: timestamp('updated_at', { withTimezone: true }).notNull().defaultNow(),​
//... other columns​

​
// Add the embedding column with the chosen dimension​
embedding: vector('embedding', { dimension: 512 }), ​

});​
​

// Other tables (users, companies, etc.) remain as defined previously​

2. Configure drizzle.config.ts for Supabase
The Drizzle Kit CLI needs to know how to connect to the Supabase database to
generate and apply migrations. This is configured in the drizzle.config.ts file at the
project root, which securely reads the database connection string from an
environment variable.10

TypeScript

// drizzle.config.ts​
import { config } from 'dotenv';​
import { defineConfig } from 'drizzle-kit';​

​

config({ path: '.env' }); // Load environment variables​
​
export default defineConfig({​
schema: './src/db/schema.ts',​
out: './supabase/migrations', // Output directory for Supabase CLI compatibility​
dialect: 'postgresql',​
dbCredentials: {​
url: process.env.DATABASE_URL!, // Connection string for Supabase​
},​
});​

3. Generate the SQL Migration with drizzle-kit
With the schema updated and the configuration in place, the developer runs a single
command to generate the necessary SQL migration file.10

Bash

npx drizzle-kit generate​
Drizzle Kit introspects the current database state, compares it to the new schema
defined in schema.ts, and automatically generates a new SQL file in the
supabase/migrations directory. This file will contain the precise DDL statement to
enact the change:

SQL

-- Example output in supabase/migrations/0001_add_embeddings.sql​
ALTER TABLE "activities" ADD COLUMN "embedding" vector(512);​

This step perfectly illustrates how the TypeScript schema serves as the single source
of truth, with the corresponding SQL being a generated artifact.

4. Apply the Migration
The generated migration can be applied to the live Supabase database in two ways 10:
1.​ Using Drizzle Kit Directly: npx drizzle-kit migrate
2.​ Using the Supabase CLI: This is often preferred for projects integrated with

Supabase's local development and CI/CD workflows. The command is supabase
db push.
Once applied, the activities table in the production database now has the embedding
column, ready to store vectors.

5. Write the Type-Safe Query in the Application
This is the final payoff of the integrated stack. The application can now query for
similar activities using the RPC function created in Part III. The entire operation, from
the database call to the final application logic, remains type-safe.
The following code shows a service function that takes an embedding and finds
similar activities. It uses Drizzle's db.execute method to call the raw SQL function and
explicitly types the result, ensuring that any downstream code using this function
benefits from TypeScript's static analysis.

TypeScript

// src/services/activitySearch.ts​
import { sql } from 'drizzle-orm';​
import { db } from '../db'; // The initialized Drizzle instance​

​

// Define the shape of the data returned by our 'match_activities' RPC function​
// This ensures type safety for the raw SQL query result.​
type MatchedActivity = {​
id: string;​

subject: string;​
notes: string;​
similarity: number;​

};​
​
/**​
* Finds activities semantically similar to a given query vector.​
* @param embedding The 512-dimension query vector.​
* @param threshold The minimum similarity score (0 to 1).​
* @param count The maximum number of results to return.​
* @returns A promise that resolves to an array of similar activities.​
*/​
export async function findSimilarActivities(​
embedding: number,​
threshold: number = 0.75,​
count: number = 10​
): Promise<MatchedActivity> {​

​
// Drizzle's 'sql' tag helps prevent SQL injection with parameters.​
const query = sql`​
SELECT id, subject, notes, similarity ​
FROM match_activities(${embedding}, ${threshold}, ${count})​
`;​

​
// Execute the RPC function and cast the result to our defined type.​
const { rows } = await db.execute(query);​

​
return rows as MatchedActivity;​

}​
This complete, type-safe workflow—from schema definition in TypeScript to a fully
typed query result in the application—is the engine of development velocity. It allows
the team to build, extend, and refactor complex AI-powered features with a high
degree of confidence and speed.

Conclusion: An Action Plan
This report has detailed an architectural blueprint for evolving a well-designed
relational database into a modern, AI-centric CRM platform. By building upon the
provided schema's strong foundation of data integrity and layering on a modern,
high-velocity technology stack, the resulting system will be positioned for rapid
innovation and long-term scalability. The following is a concise, actionable checklist to
guide execution.

1.​ Accept the Foundational Schema: Formally adopt the database design detailed

in the PostgreSQL CRM Schema Design for Performance.pdf document.1 Its
principles of normalization and data integrity are the critical bedrock for all
subsequent AI work.
2.​ Establish the Technology Stack:
○​ Create a new project on the Supabase platform.11
○​ Initialize a new Node.js project with TypeScript.
○​ Install the necessary dependencies: drizzle-orm for the query builder and
drizzle-kit as a development dependency for migrations.8
3.​ Implement the Core Schema:
○​ Translate the CREATE TABLE statements from the design document into
Drizzle's TypeScript schema syntax in a src/db/schema.ts file.10
○​ Use drizzle-kit generate to create the initial SQL migration files.
○​ Apply this initial migration to the Supabase instance using supabase db push
or drizzle-kit migrate.10
4.​ Integrate the Intelligence Layer:
○​ In the Supabase dashboard, enable the pgvector extension.7
○​ Select the embedding model: Begin with OpenAI's
text-embedding-3-large for its performance and the MRL feature.18
○​ Modify the activities table in schema.ts to add an embedding:
vector('embedding', { dimension: 512 }) column, opting for the truncated
vector size for efficiency.18
○​ In the Supabase SQL Editor, create the match_activities RPC function as
detailed in Part III.11
○​ Create an HNSW index on the new embedding column to ensure fast query
performance: CREATE INDEX ON activities USING hnsw (embedding
vector_cosine_ops);.13
5.​ Build the First AI Feature:
○​ Develop a backend service that triggers whenever a new activity record with
non-empty notes is created.
○​ This service should call the OpenAI API to generate an embedding for the
notes text.
○​ Upon receiving the 3072-dimension vector from the API, truncate it to the first
512 dimensions in the application code.
○​ Save this 512-dimension vector into the embedding column of the
corresponding activity row.
○​ Construct a user-facing search interface that calls the backend, which in turn
uses the findSimilarActivities service function from Part V to perform a
semantic search.

6.​ Schedule a Strategic Re-evaluation: Create a recurring six-month calendar

event for the technical leadership titled "Re-evaluate Embedding Model Strategy."
The purpose of this meeting is to assess API costs against the engineering cost of
implementing a self-hosted alternative. Do not deviate from the API-based
strategy until it becomes a clear financial or performance bottleneck.
Works cited
1.​ PostgreSQL CRM Schema Design for Performance.pdf
2.​ Connect to your database | Supabase Docs, accessed August 14, 2025,

https://supabase.com/docs/guides/database/connecting-to-postgres
3.​ AI & Vectors | Supabase Docs, accessed August 14, 2025,
https://supabase.com/docs/guides/ai
4.​ pgvector: Embeddings and vector similarity | Supabase Docs, accessed August
14, 2025, https://supabase.com/docs/guides/database/extensions/pgvector
5.​ The Postgres Vector database and AI Toolkit - Supabase, accessed August 14,
2025, https://supabase.com/modules/vector
6.​ Supabase Vector Database | OpenAI Cookbook, accessed August 14, 2025,
https://cookbook.openai.com/examples/vector_databases/supabase/readme
7.​ Vector columns | Supabase Docs, accessed August 14, 2025,
https://supabase.com/docs/guides/ai/vector-columns
8.​ Getting Started with Drizzle ORM | Better Stack Community, accessed August 14,
2025, https://betterstack.com/community/guides/scaling-nodejs/drizzle-orm/
9.​ Guides - Drizzle ORM, accessed August 14, 2025,
https://orm.drizzle.team/docs/guides
10.​Drizzle with Supabase Database - Drizzle ORM, accessed August 14, 2025,
https://orm.drizzle.team/docs/tutorials/drizzle-with-supabase
11.​ Build a RAG App With Descope, Supabase & pgvector: Part 1, accessed August
14, 2025,
https://www.descope.com/blog/post/rag-descope-supabase-pgvector-1
12.​Managing Indexes in PostgreSQL | Supabase Docs, accessed August 14, 2025,
https://supabase.com/docs/guides/database/postgres/indexes
13.​Vector indexes | Supabase Docs, accessed August 14, 2025,
https://supabase.com/docs/guides/ai/vector-indexes
14.​Optimizing Vector Search at Scale: Lessons from pgvector & Supabase
Performance Tuning | by Dikhyant Krishna Dalai | Jul, 2025 | Medium, accessed
August 14, 2025,
https://medium.com/@dikhyantkrishnadalai/optimizing-vector-search-at-scale-le
ssons-from-pgvector-supabase-performance-tuning-ce4ada4ba2ed
15.​Comparing Popular Embedding Models: Choosing the Right One for ..., accessed
August 14, 2025,
https://dev.to/simplr_sh/comparing-popular-embedding-models-choosing-the-ri
ght-one-for-your-use-case-43p1
16.​The Great Embedding Model Evaluation: Why We're Succeeding with the Wrong

Model | by Michael Rico | Jul, 2025 | Medium, accessed August 14, 2025,
https://medium.com/@ricomanifesto/the-great-embedding-model-evaluation-w
hy-were-succeeding-with-the-wrong-model-2a43c7352fae
17.​Models - OpenAI API, accessed August 14, 2025,
https://platform.openai.com/docs/models
18.​OpenAI's Text Embeddings v3 - Pinecone, accessed August 14, 2025,
https://www.pinecone.io/learn/openai-embeddings-v3/
19.​Exploring Text-Embedding-3-Large: A Comprehensive Guide to the new OpenAI
Embeddings | DataCamp, accessed August 14, 2025,
https://www.datacamp.com/tutorial/exploring-text-embedding-3-large-new-ope
nai-embeddings
20.​OpenAI's NEW Embedding Models - YouTube, accessed August 14, 2025,
https://www.youtube.com/watch?v=cUyw5eG-VtM
21.​Comparing Cohere, Amazon Titan, and OpenAI Embedding Models: A Deep Dive
- Medium, accessed August 14, 2025,
https://medium.com/@aniketpatil8451/comparing-cohere-amazon-titan-and-ope
nai-embedding-models-a-deep-dive-b7a5c116b6e3
22.​Text Embedding Models Compared: OpenAI, Voyage, Cohere & More Document360, accessed August 14, 2025,
https://document360.com/blog/text-embedding-model-analysis/
23.​Cohere's Embed Models (Details and Application) | Cohere, accessed August 14,
2025, https://docs.cohere.com/docs/cohere-embed
24.​Embed | Secure AI Retrieval - Cohere, accessed August 14, 2025,
https://cohere.com/embed
25.​Custom migrations - Drizzle ORM, accessed August 14, 2025,
https://orm.drizzle.team/docs/kit-custom-migrations

